{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/patrick/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/patrick/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "/home/patrick/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "#from gpt4all import GPT4All\n",
    "#import gpt4all\n",
    "#path = \"C:\\Users\\Jakob\\Downloads\\gpt4all-falcon-q4_0.gguf\"\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing of our data (creating smaller subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_file_by_lines(filename, lines_per_file,create_file):\n",
    "    with open(filename, 'r', encoding='utf-8') as file: \n",
    "        count = 0\n",
    "        current_file = open(create_file, 'w', encoding='utf-8') \n",
    "        \n",
    "        numLine = 0\n",
    "        count = 0\n",
    "        for line in file:\n",
    "            if(numLine > lines_per_file):\n",
    "                break\n",
    "            if (count % 1000) == 0:\n",
    "                current_file.write(line)\n",
    "                numLine += 1\n",
    "            count += 1    \n",
    "\n",
    "\n",
    "        current_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_file_by_lines_topic(filename, lines_per_file,create_file,topic):\n",
    "    with open(filename, 'r', encoding='utf-8') as file: \n",
    "        count = 0\n",
    "        current_file = open(create_file, 'w', encoding='utf-8') \n",
    "        \n",
    "        numLine = 0\n",
    "        count = 0\n",
    "        for line in file:\n",
    "            if(numLine > lines_per_file):\n",
    "                break\n",
    "            split_line = line.split(' ||| ')\n",
    "            if topic in split_line[1]:\n",
    "                current_file.write(line)\n",
    "                numLine += 1\n",
    "            count += 1    \n",
    "\n",
    "\n",
    "        current_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the complete titles for the queries in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_to_df_title(filename):\n",
    "    fp = open(filename, 'r', encoding='utf-8')\n",
    "    queries = []\n",
    "    for entry in fp:\n",
    "        query, _ = entry.split(' ||| ', 1)\n",
    "        queries.append(query)\n",
    "    df_queries = pd.DataFrame(queries, columns=['Query'])\n",
    "    fp.close()\n",
    "    return df_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_to_df(filename):\n",
    "    df = pd.read_csv(filename, delimiter=' \\|\\|\\| ', engine='python', header=None, names=['Title', 'Text'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = GPT4All(model_name=\"gpt4all-falcon-q4_0.gguf\",model_path=\"./model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a .txt file with the response of the llm as a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateDocWithLLM(queries):\n",
    "    fp = open(\"llmDocs.txt\", 'a',encoding='utf-8')\n",
    "    index = 0\n",
    "    start = 0\n",
    "    for title in queries['Title']:\n",
    "        if start == 1:\n",
    "            query = f'give me a short summary on {title}'\n",
    "            input = query\n",
    "            #print(input)\n",
    "            with llm.chat_session():\n",
    "                response = llm.generate(input, temp=0)\n",
    "            #print(response)\n",
    "            fp.write(str(title) + \" ||| \" + str(response) + \"\\n\")\n",
    "            # if(index == 3):\n",
    "            #     break\n",
    "        index += 1\n",
    "        if(index % 50 == 0):\n",
    "            print(index)\n",
    "        if(index == 181):\n",
    "            start = 1    \n",
    "    fp.close()\n",
    "    print(\"finished\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split_file_by_lines('data/stemmed.txt', 1000,'data/wikipediaSubset.txt')\n",
    "#split_file_by_lines('data/raw.txt', 1000,'data/textForQueries.txt')\n",
    "#df = txt_to_df('data/wikipediaSubset.txt')\n",
    "#dfQ = txt_to_df_title('data/textForQueries.txt')\n",
    "\n",
    "\n",
    "#split_file_by_lines_topic('data/raw.txt', 1000,'data/topic.txt',' sport ')\n",
    "topic_dataframe = txt_to_df('data/topic.txt')\n",
    "\n",
    "#display(df)\n",
    "#display(dfQ)\n",
    "# generateDocWithLLM(topic_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesses the data generated by the LLM as well as the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_iterator(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            split = line.split(\"|||\")\n",
    "            title = split[0]\n",
    "            #join in the rare case this sequenze occurs more than once\n",
    "            text = ''.join(map(str, split[1:]))\n",
    "            yield (title, text)\n",
    "\n",
    "def preprocess_Data(filename, isLLMData):\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    \n",
    "    df = {}\n",
    "\n",
    "    counter = 1\n",
    "    for (title, text) in file_iterator(filename):\n",
    "        \n",
    "        if isLLMData:\n",
    "            id = \"L\" + str(counter)\n",
    "        else:\n",
    "            id = \"W\" + str(counter)\n",
    "            \n",
    "        text = title + \" \" +text\n",
    "\n",
    "        #Tokenize\n",
    "        text = word_tokenize(text)\n",
    "\n",
    "        #convert words to lowercase\n",
    "        text = [t.lower() for t in text]\n",
    "\n",
    "        #remove punctuation\n",
    "        text = [t for t in text if t.isalnum() or t.isspace()]\n",
    "\n",
    "        #remove stopwords\n",
    "        text = [t for t in text if t not in stopwords_english]\n",
    "\n",
    "        #stemm\n",
    "        text = [stemmer.stem(t) for t in text]\n",
    "        \n",
    "        df[id] = text\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = preprocess_Data('llmDocs.txt', True)\n",
    "# data2 = preprocess_Data('data/topic.txt', False)\n",
    "# data.update(data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retriveing the first X documents using BM25. This is used for efficiently eliminating unreleveant documents. The resulting documaents are later reranked using more computaion intensive but also more acurate methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.k1 = 1.5\n",
    "        self.b = 0.75\n",
    "        self.inverted_index = self.calc_inverted_index()\n",
    "        self.bm25 = self.calc_bm25_matrix()\n",
    "\n",
    "\n",
    "    def calc_inverted_index(self) -> dict[str, list[str]]:\n",
    "\n",
    "        inverted_index = {}\n",
    "        \n",
    "        for doc_id, words in self.data.items():\n",
    "            for word in words:\n",
    "                if word not in inverted_index.keys():\n",
    "                    inverted_index[word] = []\n",
    "                    inverted_index[word].append(doc_id)\n",
    "                    \n",
    "                else:\n",
    "                    if doc_id not in inverted_index[word]:\n",
    "                        inverted_index[word].append(doc_id)\n",
    "\n",
    "        return inverted_index\n",
    "\n",
    "\n",
    "    def calc_bm25_matrix(self):\n",
    "            \n",
    "        bm25 = {}     \n",
    "\n",
    "        #construct vector for every word\n",
    "        average_document_length = self.average_document_length()\n",
    "        for doc_id, words in self.data.items():\n",
    "            bm25[doc_id] = {}\n",
    "            document_unique = np.unique(words)\n",
    "            document = words\n",
    "            documnet_id = doc_id\n",
    "            for word in document_unique:\n",
    "                idf = self.calculate_idf(len(self.inverted_index[word]))\n",
    "                self.calc_bm25(word, bm25, idf, average_document_length, document, documnet_id)\n",
    "\n",
    "        return bm25\n",
    "\n",
    "    def calc_bm25(self, word, bm25, idf, average_document_length, document, documnet_id):\n",
    "        \n",
    "        term_frequency = document.count(word)\n",
    "        \n",
    "        numerator = idf * term_frequency * (self.k1 + 1)\n",
    "        denominator = term_frequency + self.k1 * (1 - self.b + ((self.b * len(document)) / average_document_length))\n",
    "        bm25[documnet_id][word] = numerator / denominator\n",
    "\n",
    "    def average_document_length(self):\n",
    "        element_counter = 0\n",
    "        word_counter = 0\n",
    "        for _, words in self.data.items():\n",
    "            element_counter += 1\n",
    "            word_counter += len(words)\n",
    "\n",
    "        return word_counter / element_counter\n",
    "    \n",
    "    def calculate_idf(self, amount_documents_including_word):\n",
    "        return np.log(len(self.data) / amount_documents_including_word)\n",
    "    \n",
    "    def preprocess_query(self, query: str):\n",
    "        stemmer = PorterStemmer()\n",
    "        stopwords_english = stopwords.words('english')\n",
    "\n",
    "        #Tokenize\n",
    "        query = word_tokenize(query)\n",
    "\n",
    "        #convert words to lowercase\n",
    "        query = [t.lower() for t in query]\n",
    "\n",
    "        #remove punctuation\n",
    "        query = [t for t in query if t.isalnum or t.isspace()]\n",
    "\n",
    "        #remove stopwords\n",
    "        query = [t for t in query if t not in stopwords_english]\n",
    "\n",
    "        #stemm\n",
    "        query = [stemmer.stem(t) for t in query]\n",
    "\n",
    "        return query\n",
    "\n",
    "\n",
    "    def retrieve_relevance(self, query: str, k: int) -> dict[str, float]:\n",
    "        ### ADD YOUR CODE (BEGIN) ###\n",
    "\n",
    "        query = self.preprocess_query(query)\n",
    "\n",
    "        #filter out word not contained in any document for efficiency\n",
    "        query = [word for word in query if word in self.inverted_index.keys()]\n",
    "\n",
    "        bm25_current_query = {}\n",
    "\n",
    "        for doc_id, words in self.data.items():\n",
    "            bm25_current_query[doc_id] = []\n",
    "            for word in query:\n",
    "                if word in self.bm25[doc_id]:\n",
    "                    bm25_current_query[doc_id].append(self.bm25[doc_id][word])\n",
    "                else: \n",
    "                    bm25_current_query[doc_id].append(0)\n",
    "\n",
    "        for key, value in bm25_current_query.items():\n",
    "            bm25_current_query[key] = sum(value)\n",
    "\n",
    "        sorted_bm25 = sorted(bm25_current_query.items(), key=lambda x:x[1], reverse=True)\n",
    "\n",
    "        sorted_bm25 = sorted_bm25[:k]\n",
    "        bm25_current_query = dict(sorted_bm25)\n",
    "\n",
    "        return bm25_current_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bm25 = BM25(data)\n",
    "\n",
    "#print(bm25.retrieve_relevance(\"Formula\", 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Rerank using MonoBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./model/tokenizer_config.json',\n",
       " './model/special_tokens_map.json',\n",
       " './model/vocab.txt',\n",
       " './model/added_tokens.json',\n",
       " './model/tokenizer.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download model\n",
    "\n",
    "model_name = \"castorini/monobert-large-msmarco\"\n",
    "model_path = \"./model\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.save_pretrained(model_path)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./model\"\n",
    "\n",
    "local_model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "local_tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevance_bert(query, documents):\n",
    "    relevances = []\n",
    "    for document in documents:\n",
    "        \n",
    "        inputs = tokenizer(query, document[1], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "        # predictions    \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # e.g.:\n",
    "        # outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-1.8822,  2.8848]]), hidden_states=None, attentions=None)\n",
    "        # logits: tensor([[-1.8822,  2.8848]])\n",
    "\n",
    "        # re-scale to [0,1] and so that sum == 1\n",
    "        relevance = torch.nn.functional.softmax(logits, dim=1)\n",
    "        relevances.append([document[0], relevance[0].tolist()[1], document[1]])\n",
    "\n",
    "    return relevances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_relevance(relevances):\n",
    "    relevances = sorted(relevances, reverse=True, key=lambda x: x[1])\n",
    "    return relevances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"What is the capital of France?\"\n",
    "# documents = [\"Paris is the capital of France.\",\n",
    "#              \"birds like France.\",\n",
    "#              \"snakes eat gras\",\n",
    "#              \"Graz is the capital of France.\",\n",
    "#              \"France is capital paris\",\n",
    "#              \"paris is capital france\",\n",
    "#              \"France capital paris egg\"]\n",
    "\n",
    "# relevances = get_relevance_bert(query, documents)\n",
    "# relevances = order_relevance(relevances)\n",
    "\n",
    "# for relevance in relevances:\n",
    "#     print(relevance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full retrival pipeline using BM25 and BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    def __init__(self, amount_bm25_fetched_documents, amount_of_queries):\n",
    "        data=self.getData()\n",
    "        self.bm25 = BM25(data)\n",
    "        self.documents = self.getCompleteFiles()\n",
    "        self.amount_bm25_fetched_documents = amount_bm25_fetched_documents\n",
    "        self.queries = self.get_queries_from_doc()\n",
    "        self.amount_of_queries = amount_of_queries\n",
    "\n",
    "    def getData(self):\n",
    "        data = preprocess_Data('llmDocs.txt', True)\n",
    "        data2 = preprocess_Data('data/topic.txt', False)\n",
    "        data |= data2\n",
    "        return data\n",
    "    \n",
    "    def getCompleteFiles(self):\n",
    "        documents = {}\n",
    "\n",
    "        #read LLM generated Documents\n",
    "        counter = 1\n",
    "        with open('llmDocs.txt', 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                split = line.split(\"|||\")\n",
    "                #join in the rare case this sequenze occurs more than once\n",
    "                text = ''.join(map(str, split[1:]))\n",
    "                documents[\"L\"+str(counter)] = text\n",
    "                counter+=1\n",
    "\n",
    "        #read original wikipedia documents\n",
    "        counter = 1\n",
    "        with open('data/topic.txt', 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                split = line.split(\"|||\")\n",
    "                #join in the rare case this sequenze occurs more than once\n",
    "                text = ''.join(map(str, split[1:]))\n",
    "                documents[\"W\"+str(counter)] = text\n",
    "                counter+=1\n",
    "        \n",
    "        return documents\n",
    "    \n",
    "    def retrieve_relevant_documents_with_BERT(self, query, documents):\n",
    "        relevances = get_relevance_bert(query, documents)\n",
    "        relevances = order_relevance(relevances)   \n",
    "        return relevances \n",
    "    \n",
    "    def retrieve_relevant_documents(self, query):\n",
    "\n",
    "        #retrieve amount_bm25_fetched_documents documents using fast bm25\n",
    "        bm25_docs = self.bm25.retrieve_relevance(query, self.amount_bm25_fetched_documents)\n",
    "\n",
    "        #fetch the non preprocessed documents retrieved by bm25\n",
    "        retrieved_documents = []\n",
    "        for doc_id in bm25_docs:\n",
    "            retrieved_documents.append([doc_id, self.documents[doc_id]])\n",
    "\n",
    "        #rerank these documents using BERT\n",
    "        ranked_document_list = self.retrieve_relevant_documents_with_BERT(query, retrieved_documents)\n",
    "\n",
    "        return ranked_document_list\n",
    "\n",
    "    def get_queries_from_doc(self):\n",
    "        queries_list = []\n",
    "        with open('data/topic.txt', 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                split = line.split(\"|||\")\n",
    "                queries_list.append(split[0])\n",
    "        return queries_list\n",
    "    \n",
    "    def calculate_metrics(self):\n",
    "        ranked_document_list = []\n",
    "        for query in self.queries[:self.amount_of_queries]:\n",
    "            curr_results = self.retrieve_relevant_documents(query)\n",
    "            print(curr_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Auto racing ', [['W1', 0.9991298317909241, ' Auto racing (also known as car racing, motor racing, or automobile racing) is a sport involving the racing of automobiles for competition. Almost as soon as automobiles had been invented, races of various sorts were organised, with the first recorded as early as 1867. Many of the earliest events were effectively reliability trials, aimed at proving these new machines were a practical mode of transport, but soon became an important way for competing makers to demonstrate their machines. By the 1930s specialist racing cars had developed. There are now numerous different categories, each with different rules and regulations.\\n'], ['L1', 0.9982098340988159, ' Auto racing is a competitive sport that involves driving high-performance vehicles around a track. The most popular form of auto racing is Formula One, which features single-seat open-wheel cars and is considered the pinnacle of the sport. Other forms of auto racing include NASCAR, IndyCar, and rally racing. Auto racing requires skill, precision, and speed to compete at the highest levels.\\n'], ['W414', 0.9170028567314148, ' The National Electric Drag Racing Association (NEDRA), a Special Chapter of the Electric Auto Association, and exists to increase public awareness of electric vehicle (EV) performance and to encourage through competition, advances in electric vehicle technology. NEDRA achieves this by organizing and sanctioning safe, silent and exciting electric vehicle drag racing events. NEDRA is a coalition of drag racing fans, electric drag racing vehicle owners and drivers, individuals interested in promoting the sport of EV drag racing, EV parts suppliers, EV manufacturers and other environmentally concerned companies and individuals. Working together as a group, we put excitement into electric vehicle drag racing. \\n']]]\n",
      "[\"America's National Game \", [['W2', 0.9993897676467896, \" America's National Game is a book by Albert Spalding, published in 1911 detailing the early history of the sport of baseball. Much of the story is told first-hand, since Spalding had been involved in the game, first as a player and later an administrator, since the 1850s. In addition to his personal recollections, he had access to the records of Henry Chadwick, the game's first statistician and archivist. Spalding was, however, known to aggrandise his role in the major moments in baseball's history.\\n\"], ['L2', 0.9988468885421753, \" America's national game is baseball. It is played by two teams, each with nine players, and the objective is to score runs by hitting a ball with a bat and running around four bases in order. The game originated in the United States in the 1800s and has since become one of the most popular sports in the country.\\n\"], ['L787', 3.43139108736068e-05, \" The Colombia national football team, also known as La Tricolor, is the national football team of Colombia. It competes in international competitions such as the FIFA World Cup and Copa America. The team has had success in recent years, reaching the quarter-finals of the 2014 FIFA World Cup and winning the Copa America Centenario in 2016. The team's star player is striker Radamel Falcao, who has scored over 50 goals for Colombia.\\n\"]]]\n",
      "['Archery ', [['W3', 0.9977218508720398, ' Archery is the sport, practice or skill of using a bow to propel arrows. The word comes from the Latin arcus. Historically, archery has been used for hunting and combat. In modern times, it is mainly a competitive sport and recreational activity. A person who participates in archery is typically called an archer or a bowman, and a person who is fond of or an expert at archery is sometimes called a toxophilite.\\n'], ['L3', 0.9955755472183228, ' Archery is an ancient sport that involves shooting arrows at targets using a bow and arrow. It has been practiced for thousands of years, with its origins dating back to ancient civilizations such as the Egyptians, Greeks, and Romans. Today, archery is still enjoyed by many people around the world as a recreational activity or competitive sport.\\n'], ['W878', 0.08886729925870895, \" Justin Grant Huish (born January 9, 1975 in Fountain Valley, California) is an American double Olympic champion in Archery, titles he won at the 1996 Atlanta Games. Known for his unorthodox character sporting wrap-around shades, backwards baseball cap, ponytail and an earring, Huish captivated the crowd by winning a gold medal in the individual and team events. In the individual, Huish ranked no higher than 24th in the world at the time but rose to the occasion and saw off Sweden’s Magnus Pettersson 112-107 in the final. Huish was the first male archer to win double gold medals and his wins led to a popularity boost for the sport. His feat was matched by South Korea's Ku Bon-chan in the 2016 Summer Olympics. He was later credited for encouraging actress Geena Davis to pick up the sport. As a ninth-grader he was into skateboarding, lived on a street called Broken Arrow, and thought archery was boring. But when Huish's parents opened Arrowsmith Archery, he wanted a job. He didn't pick up the sport until age 14 and would practice standing in the middle of the street, shooting arrows through the front door of his garage, through the back door of his garage, and into a target in his backyard. Only three years after taking up a bow, 17-year-old Justin Huish won the intermediate age division at the National Archery Assn.'s Outdoor Target Championships, significant victories at the 1993 US collegiate championships and the 1995 US Target Championships. He had only been on the US National Team since 1994.\\n\"]]]\n"
     ]
    }
   ],
   "source": [
    "pip = Pipeline(3, 3)\n",
    "\n",
    "# ranked_document_list = pip.retrieve_relevant_documents(\"formula one is no fun\")\n",
    "# for doc in ranked_document_list:\n",
    "#    print(doc)\n",
    "\n",
    "pip.calculate_metrics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
