{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing of our data (creating smaller subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_file_by_lines(filename, lines_per_file):\n",
    "    with open(filename, 'r', encoding='utf-8') as file: \n",
    "        count = 0\n",
    "        current_file = open('data/wikipediaSubset.txt', 'w', encoding='utf-8') \n",
    "        \n",
    "        numLine = 0\n",
    "        count = 0\n",
    "        for line in file:\n",
    "            if(numLine > lines_per_file):\n",
    "                break\n",
    "            if (count % 1000) == 0:\n",
    "                current_file.write(line)\n",
    "                numLine += 1\n",
    "            count += 1    \n",
    "\n",
    "\n",
    "        current_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_to_df(filename):\n",
    "    df = pd.read_csv(filename, delimiter=' \\|\\|\\| ', engine='python', header=None, names=['Title', 'Text'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jakob\\AppData\\Local\\Temp\\ipykernel_11940\\3461223430.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(filename, sep=\"|||\")\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Expected 848 fields in line 3, saw 1277. Error could possibly be due to quotes being ignored when a multi-char delimiter is used.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#split_file_by_lines('data/stemmed.txt', 1000)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mtxt_to_df\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/wikipediaSubset.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m display(df)\n",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m, in \u001b[0;36mtxt_to_df\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtxt_to_df\u001b[39m(filename):\n\u001b[1;32m----> 2\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m|||\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[1;32mc:\\Users\\Jakob\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jakob\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:617\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jakob\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1748\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1741\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1743\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m     (\n\u001b[0;32m   1745\u001b[0m         index,\n\u001b[0;32m   1746\u001b[0m         columns,\n\u001b[0;32m   1747\u001b[0m         col_dict,\n\u001b[1;32m-> 1748\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1749\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1750\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1751\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1752\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\Jakob\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\python_parser.py:286\u001b[0m, in \u001b[0;36mPythonParser.read\u001b[1;34m(self, rows)\u001b[0m\n\u001b[0;32m    283\u001b[0m     indexnamerow \u001b[38;5;241m=\u001b[39m content[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    284\u001b[0m     content \u001b[38;5;241m=\u001b[39m content[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m--> 286\u001b[0m alldata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rows_to_cols\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    287\u001b[0m data, columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exclude_implicit_index(alldata)\n\u001b[0;32m    289\u001b[0m conv_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_data(data)\n",
      "File \u001b[1;32mc:\\Users\\Jakob\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\python_parser.py:1058\u001b[0m, in \u001b[0;36mPythonParser._rows_to_cols\u001b[1;34m(self, content)\u001b[0m\n\u001b[0;32m   1052\u001b[0m             reason \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1053\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError could possibly be due to quotes being \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1054\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignored when a multi-char delimiter is used.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1055\u001b[0m             )\n\u001b[0;32m   1056\u001b[0m             msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m reason\n\u001b[1;32m-> 1058\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_alert_malformed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow_num\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;66;03m# see gh-13320\u001b[39;00m\n\u001b[0;32m   1061\u001b[0m zipped_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(lib\u001b[38;5;241m.\u001b[39mto_object_array(content, min_width\u001b[38;5;241m=\u001b[39mcol_len)\u001b[38;5;241m.\u001b[39mT)\n",
      "File \u001b[1;32mc:\\Users\\Jakob\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\python_parser.py:779\u001b[0m, in \u001b[0;36mPythonParser._alert_malformed\u001b[1;34m(self, msg, row_num)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    763\u001b[0m \u001b[38;5;124;03mAlert a user about a malformed row, depending on value of\u001b[39;00m\n\u001b[0;32m    764\u001b[0m \u001b[38;5;124;03m`self.on_bad_lines` enum.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    776\u001b[0m \u001b[38;5;124;03m    even though we 0-index internally.\u001b[39;00m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_bad_lines \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBadLineHandleMethod\u001b[38;5;241m.\u001b[39mERROR:\n\u001b[1;32m--> 779\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ParserError(msg)\n\u001b[0;32m    780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_bad_lines \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBadLineHandleMethod\u001b[38;5;241m.\u001b[39mWARN:\n\u001b[0;32m    781\u001b[0m     base \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkipping line \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mParserError\u001b[0m: Expected 848 fields in line 3, saw 1277. Error could possibly be due to quotes being ignored when a multi-char delimiter is used."
     ]
    }
   ],
   "source": [
    "#split_file_by_lines('data/stemmed.txt', 1000)\n",
    "df = txt_to_df('data/wikipediaSubset.txt')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocesses the data generated by the LLM in the same way as the original data (https://github.com/tscheepers/Wikipedia-Summary-Dataset/blob/master/src/process.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_iterator():\n",
    "    with open('LLM-Output.txt', 'r') as file:\n",
    "        for line in file:\n",
    "            split = line.split(\"|||\")\n",
    "            title = split[0]\n",
    "            #join in the rare case this sequenze occurs more than once\n",
    "            text = ''.join(map(str, split[1:]))\n",
    "            yield (title, text)\n",
    "\n",
    "def preprocess_LLM_output():\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    punctiation_words = ['.', ',', ';', ':', '(', ')', '`', '\\'', '\\'\\'', '-', '–', '—', '…', '[', ']', '...', '{', '}']\n",
    "    forbidden_words = ['']\n",
    "    punctiation_chars = ['`']\n",
    "    df = {'title': [], 'text': [] }\n",
    "    for (title, text) in file_iterator():\n",
    "\n",
    "        #Tokenize\n",
    "        title = word_tokenize(title)\n",
    "        text = word_tokenize(text)\n",
    "\n",
    "        #convert words to lowercase\n",
    "        title = [t.lower() for t in title]\n",
    "        text = [t.lower() for t in text]\n",
    "\n",
    "        #remove punctuation\n",
    "        title = [t for t in title if t not in punctiation_words]\n",
    "        text = [t for t in text if t not in punctiation_words]\n",
    "\n",
    "        #remove punctuation chars\n",
    "        title = [''.join(c for c in t if c not in punctiation_chars) for t in title]\n",
    "        text = [''.join(c for c in t if c not in punctiation_chars) for t in text]\n",
    "\n",
    "        #remove forbidden words\n",
    "        title = [t for t in title if t not in forbidden_words]\n",
    "        text = [t for t in text if t not in forbidden_words]\n",
    "\n",
    "        #remove stopwords\n",
    "        title = [t for t in title if t not in stopwords_english]\n",
    "        text = [t for t in text if t not in stopwords_english]\n",
    "\n",
    "        #stemm\n",
    "        title = [stemmer.stem(t) for t in title]\n",
    "        text = [stemmer.stem(t) for t in text]\n",
    "\n",
    "        df['title'].apped(title)\n",
    "        df['text'].append(text)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
