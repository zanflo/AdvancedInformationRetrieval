{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/patrick/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/patrick/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "/home/patrick/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "#from gpt4all import GPT4All\n",
    "#import gpt4all\n",
    "#path = \"C:\\Users\\Jakob\\Downloads\\gpt4all-falcon-q4_0.gguf\"\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing of our data (creating smaller subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_file_by_lines(filename, lines_per_file,create_file):\n",
    "    with open(filename, 'r', encoding='utf-8') as file: \n",
    "        count = 0\n",
    "        current_file = open(create_file, 'w', encoding='utf-8') \n",
    "        \n",
    "        numLine = 0\n",
    "        count = 0\n",
    "        for line in file:\n",
    "            if(numLine > lines_per_file):\n",
    "                break\n",
    "            if (count % 1000) == 0:\n",
    "                current_file.write(line)\n",
    "                numLine += 1\n",
    "            count += 1    \n",
    "\n",
    "\n",
    "        current_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_file_by_lines_topic(filename, lines_per_file,create_file,topic):\n",
    "    with open(filename, 'r', encoding='utf-8') as file: \n",
    "        count = 0\n",
    "        current_file = open(create_file, 'w', encoding='utf-8') \n",
    "        \n",
    "        numLine = 0\n",
    "        count = 0\n",
    "        for line in file:\n",
    "            if(numLine > lines_per_file):\n",
    "                break\n",
    "            split_line = line.split(' ||| ')\n",
    "            if topic in split_line[1]:\n",
    "                current_file.write(line)\n",
    "                numLine += 1\n",
    "            count += 1    \n",
    "\n",
    "\n",
    "        current_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the complete titles for the queries in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_to_df_title(filename):\n",
    "    fp = open(filename, 'r', encoding='utf-8')\n",
    "    queries = []\n",
    "    for entry in fp:\n",
    "        query, _ = entry.split(' ||| ', 1)\n",
    "        queries.append(query)\n",
    "    df_queries = pd.DataFrame(queries, columns=['Query'])\n",
    "    fp.close()\n",
    "    return df_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_to_df(filename):\n",
    "    df = pd.read_csv(filename, delimiter=' \\|\\|\\| ', engine='python', header=None, names=['Title', 'Text'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = GPT4All(model_name=\"gpt4all-falcon-q4_0.gguf\",model_path=\"./model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a .txt file with the response of the llm as a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateDocWithLLM(queries):\n",
    "    fp = open(\"llmDocs.txt\", 'a',encoding='utf-8')\n",
    "    index = 0\n",
    "    start = 0\n",
    "    for title in queries['Title']:\n",
    "        if start == 1:\n",
    "            query = f'give me a short summary on {title}'\n",
    "            input = query\n",
    "            #print(input)\n",
    "            with llm.chat_session():\n",
    "                response = llm.generate(input, temp=0)\n",
    "            #print(response)\n",
    "            fp.write(str(title) + \" ||| \" + str(response) + \"\\n\")\n",
    "            # if(index == 3):\n",
    "            #     break\n",
    "        index += 1\n",
    "        if(index % 50 == 0):\n",
    "            print(index)\n",
    "        if(index == 181):\n",
    "            start = 1    \n",
    "    fp.close()\n",
    "    print(\"finished\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split_file_by_lines('data/stemmed.txt', 1000,'data/wikipediaSubset.txt')\n",
    "#split_file_by_lines('data/raw.txt', 1000,'data/textForQueries.txt')\n",
    "#df = txt_to_df('data/wikipediaSubset.txt')\n",
    "#dfQ = txt_to_df_title('data/textForQueries.txt')\n",
    "\n",
    "\n",
    "#split_file_by_lines_topic('data/raw.txt', 1000,'data/topic.txt',' sport ')\n",
    "topic_dataframe = txt_to_df('data/topic.txt')\n",
    "\n",
    "#display(df)\n",
    "#display(dfQ)\n",
    "# generateDocWithLLM(topic_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesses the data generated by the LLM as well as the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_iterator(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            split = line.split(\"|||\")\n",
    "            title = split[0]\n",
    "            #join in the rare case this sequenze occurs more than once\n",
    "            text = ''.join(map(str, split[1:]))\n",
    "            yield (title, text)\n",
    "\n",
    "def preprocess_Data(filename, isLLMData):\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    \n",
    "    df = {}\n",
    "\n",
    "    counter = 1\n",
    "    for (title, text) in file_iterator(filename):\n",
    "        \n",
    "        if isLLMData:\n",
    "            id = \"L\" + str(counter)\n",
    "        else:\n",
    "            id = \"W\" + str(counter)\n",
    "            \n",
    "        text = title + \" \" +text\n",
    "\n",
    "        #Tokenize\n",
    "        text = word_tokenize(text)\n",
    "\n",
    "        #convert words to lowercase\n",
    "        text = [t.lower() for t in text]\n",
    "\n",
    "        #remove punctuation\n",
    "        text = [t for t in text if t.isalnum() or t.isspace()]\n",
    "\n",
    "        #remove stopwords\n",
    "        text = [t for t in text if t not in stopwords_english]\n",
    "\n",
    "        #stemm\n",
    "        text = [stemmer.stem(t) for t in text]\n",
    "        \n",
    "        df[id] = text\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = preprocess_Data('llmDocs.txt', True)\n",
    "# data2 = preprocess_Data('data/topic.txt', False)\n",
    "# data.update(data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retriveing the first X documents using BM25. This is used for efficiently eliminating unreleveant documents. The resulting documaents are later reranked using more computaion intensive but also more acurate methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.k1 = 1.5\n",
    "        self.b = 0.75\n",
    "        self.inverted_index = self.calc_inverted_index()\n",
    "        self.bm25 = self.calc_bm25_matrix()\n",
    "\n",
    "\n",
    "    def calc_inverted_index(self) -> dict[str, list[str]]:\n",
    "\n",
    "        inverted_index = {}\n",
    "        \n",
    "        for doc_id, words in self.data.items():\n",
    "            for word in words:\n",
    "                if word not in inverted_index.keys():\n",
    "                    inverted_index[word] = []\n",
    "                    inverted_index[word].append(doc_id)\n",
    "                    \n",
    "                else:\n",
    "                    if doc_id not in inverted_index[word]:\n",
    "                        inverted_index[word].append(doc_id)\n",
    "\n",
    "        return inverted_index\n",
    "\n",
    "\n",
    "    def calc_bm25_matrix(self):\n",
    "            \n",
    "        bm25 = {}     \n",
    "\n",
    "        #construct vector for every word\n",
    "        average_document_length = self.average_document_length()\n",
    "        for doc_id, words in self.data.items():\n",
    "            bm25[doc_id] = {}\n",
    "            document_unique = np.unique(words)\n",
    "            document = words\n",
    "            documnet_id = doc_id\n",
    "            for word in document_unique:\n",
    "                idf = self.calculate_idf(len(self.inverted_index[word]))\n",
    "                self.calc_bm25(word, bm25, idf, average_document_length, document, documnet_id)\n",
    "\n",
    "        return bm25\n",
    "\n",
    "    def calc_bm25(self, word, bm25, idf, average_document_length, document, documnet_id):\n",
    "        \n",
    "        term_frequency = document.count(word)\n",
    "        \n",
    "        numerator = idf * term_frequency * (self.k1 + 1)\n",
    "        denominator = term_frequency + self.k1 * (1 - self.b + ((self.b * len(document)) / average_document_length))\n",
    "        bm25[documnet_id][word] = numerator / denominator\n",
    "\n",
    "    def average_document_length(self):\n",
    "        element_counter = 0\n",
    "        word_counter = 0\n",
    "        for _, words in self.data.items():\n",
    "            element_counter += 1\n",
    "            word_counter += len(words)\n",
    "\n",
    "        return word_counter / element_counter\n",
    "    \n",
    "    def calculate_idf(self, amount_documents_including_word):\n",
    "        return np.log(len(self.data) / amount_documents_including_word)\n",
    "    \n",
    "    def preprocess_query(self, query: str):\n",
    "        stemmer = PorterStemmer()\n",
    "        stopwords_english = stopwords.words('english')\n",
    "\n",
    "        #Tokenize\n",
    "        query = word_tokenize(query)\n",
    "\n",
    "        #convert words to lowercase\n",
    "        query = [t.lower() for t in query]\n",
    "\n",
    "        #remove punctuation\n",
    "        query = [t for t in query if t.isalnum or t.isspace()]\n",
    "\n",
    "        #remove stopwords\n",
    "        query = [t for t in query if t not in stopwords_english]\n",
    "\n",
    "        #stemm\n",
    "        query = [stemmer.stem(t) for t in query]\n",
    "\n",
    "        return query\n",
    "\n",
    "\n",
    "    def retrieve_relevance(self, query: str, k: int) -> dict[str, float]:\n",
    "        ### ADD YOUR CODE (BEGIN) ###\n",
    "\n",
    "        query = self.preprocess_query(query)\n",
    "\n",
    "        #filter out word not contained in any document for efficiency\n",
    "        query = [word for word in query if word in self.inverted_index.keys()]\n",
    "\n",
    "        bm25_current_query = {}\n",
    "\n",
    "        for doc_id, words in self.data.items():\n",
    "            bm25_current_query[doc_id] = []\n",
    "            for word in query:\n",
    "                if word in self.bm25[doc_id]:\n",
    "                    bm25_current_query[doc_id].append(self.bm25[doc_id][word])\n",
    "                else: \n",
    "                    bm25_current_query[doc_id].append(0)\n",
    "\n",
    "        for key, value in bm25_current_query.items():\n",
    "            bm25_current_query[key] = sum(value)\n",
    "\n",
    "        sorted_bm25 = sorted(bm25_current_query.items(), key=lambda x:x[1], reverse=True)\n",
    "\n",
    "        sorted_bm25 = sorted_bm25[:k]\n",
    "        bm25_current_query = dict(sorted_bm25)\n",
    "\n",
    "        return bm25_current_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bm25 = BM25(data)\n",
    "\n",
    "#print(bm25.retrieve_relevance(\"Formula\", 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Rerank using MonoBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./model/tokenizer_config.json',\n",
       " './model/special_tokens_map.json',\n",
       " './model/vocab.txt',\n",
       " './model/added_tokens.json',\n",
       " './model/tokenizer.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download model\n",
    "\n",
    "model_name = \"castorini/monobert-large-msmarco\"\n",
    "model_path = \"./model\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.save_pretrained(model_path)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./model\"\n",
    "\n",
    "local_model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "local_tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevance_bert(query, documents):\n",
    "    relevances = []\n",
    "    for document in documents:\n",
    "        \n",
    "        inputs = tokenizer(query, document[1], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "        # predictions    \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # e.g.:\n",
    "        # outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-1.8822,  2.8848]]), hidden_states=None, attentions=None)\n",
    "        # logits: tensor([[-1.8822,  2.8848]])\n",
    "\n",
    "        # re-scale to [0,1] and so that sum == 1\n",
    "        relevance = torch.nn.functional.softmax(logits, dim=1)\n",
    "        relevances.append([document[0], relevance[0].tolist()[1], document[1]])\n",
    "\n",
    "    return relevances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_relevance(relevances):\n",
    "    relevances = sorted(relevances, reverse=True, key=lambda x: x[1])\n",
    "    return relevances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"What is the capital of France?\"\n",
    "# documents = [\"Paris is the capital of France.\",\n",
    "#              \"birds like France.\",\n",
    "#              \"snakes eat gras\",\n",
    "#              \"Graz is the capital of France.\",\n",
    "#              \"France is capital paris\",\n",
    "#              \"paris is capital france\",\n",
    "#              \"France capital paris egg\"]\n",
    "\n",
    "# relevances = get_relevance_bert(query, documents)\n",
    "# relevances = order_relevance(relevances)\n",
    "\n",
    "# for relevance in relevances:\n",
    "#     print(relevance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full retrival pipeline using BM25 and BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    def __init__(self, amount_bm25_fetched_documents):\n",
    "        data=self.getData()\n",
    "        self.bm25 = BM25(data)\n",
    "        self.documents = self.getCompleteFiles()\n",
    "        self.amount_bm25_fetched_documents = amount_bm25_fetched_documents\n",
    "        self.queries = self.get_queries_from_doc()\n",
    "\n",
    "    def getData(self):\n",
    "        data = preprocess_Data('llmDocs.txt', True)\n",
    "        data2 = preprocess_Data('data/topic.txt', False)\n",
    "        data |= data2\n",
    "        return data\n",
    "    \n",
    "    def getCompleteFiles(self):\n",
    "        documents = {}\n",
    "\n",
    "        #read LLM generated Documents\n",
    "        counter = 1\n",
    "        with open('llmDocs.txt', 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                split = line.split(\"|||\")\n",
    "                #join in the rare case this sequenze occurs more than once\n",
    "                text = ''.join(map(str, split[1:]))\n",
    "                documents[\"L\"+str(counter)] = text\n",
    "                counter+=1\n",
    "\n",
    "        #read original wikipedia documents\n",
    "        counter = 1\n",
    "        with open('data/topic.txt', 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                split = line.split(\"|||\")\n",
    "                #join in the rare case this sequenze occurs more than once\n",
    "                text = ''.join(map(str, split[1:]))\n",
    "                documents[\"W\"+str(counter)] = text\n",
    "                counter+=1\n",
    "        \n",
    "        return documents\n",
    "    \n",
    "    def retrieve_relevant_documents_with_BERT(self, query, documents):\n",
    "        relevances = get_relevance_bert(query, documents)\n",
    "        relevances = order_relevance(relevances)   \n",
    "        return relevances \n",
    "    \n",
    "    def retrieve_relevant_documents(self, query):\n",
    "\n",
    "        #retrieve amount_bm25_fetched_documents documents using fast bm25\n",
    "        bm25_docs = self.bm25.retrieve_relevance(query, self.amount_bm25_fetched_documents)\n",
    "\n",
    "        #fetch the non preprocessed documents retrieved by bm25\n",
    "        retrieved_documents = []\n",
    "        for doc_id in bm25_docs:\n",
    "            retrieved_documents.append([doc_id, self.documents[doc_id]])\n",
    "\n",
    "        #rerank these documents using BERT\n",
    "        ranked_document_list = self.retrieve_relevant_documents_with_BERT(query, retrieved_documents)\n",
    "\n",
    "        return ranked_document_list\n",
    "\n",
    "    def get_queries_from_doc(self):\n",
    "        queries_list = []\n",
    "        with open('data/topic.txt', 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                split = line.split(\"|||\")\n",
    "                queries_list.append(split[0])\n",
    "        return queries_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['L391', \" The List of Formula One circuits is a comprehensive list of all the tracks that have hosted Formula One races since its inception in 1950. The list includes both permanent and temporary circuits, as well as those that have been used for testing purposes. The list also includes information on each circuit's location, layout, and other relevant details. The list is an essential resource for fans of the sport who want to learn more about the history and evolution of Formula One tracks.\\n\"], ['L379', \" BMW is a German luxury car manufacturer that has been involved in Formula One racing since the 1980s. The company has won numerous championships and races, including the Constructors' Championship in 2001 and 2002. BMW also had a successful partnership with Sauber from 1991 to 2005, during which time they won several races and podium finishes. In recent years, BMW has been involved in Formula One as an engine supplier for teams such as Sauber, Williams, and McLaren.\\n\"], ['W522', ' Bernard Charles \"Bernie\" Ecclestone (born 28 October 1930) is a British business magnate. He was the chief executive of the Formula One Group, which manages Formula One and controls the commercial rights to the sport, and part-owns Delta Topco, the ultimate parent company of the Formula One Group. As such, he was commonly described in journalism as \\'F1 Supremo\\'. On 23 January 2017, it was announced that Ecclestone had been replaced by Chase Carey as chief executive of the Formula One Group, though he has been appointed as chairman emeritus and will act as an adviser to the board. His early involvement in the sport was as a competitor and then as a manager of drivers Stuart Lewis-Evans and Jochen Rindt. In 1972, he bought the Brabham team, which he ran for fifteen years. As a team owner he became a member of the Formula One Constructors Association. His control of the sport, which grew from his pioneering the sale of television rights in the late 1970s, was chiefly financial, but under the terms of the Concorde Agreement he and his companies also managed the administration, setup and logistics of each Formula One Grand Prix, making him one of the richest men in the UK. Ecclestone entered two Grand Prix races as a driver, during the 1958 season, but failed to qualify for either of them. Ecclestone and business partner Flavio Briatore also owned the English football club Queens Park Rangers between 2007 and 2011.\\n']]\n",
      "['L391', 0.0013528411509469151, \" The List of Formula One circuits is a comprehensive list of all the tracks that have hosted Formula One races since its inception in 1950. The list includes both permanent and temporary circuits, as well as those that have been used for testing purposes. The list also includes information on each circuit's location, layout, and other relevant details. The list is an essential resource for fans of the sport who want to learn more about the history and evolution of Formula One tracks.\\n\"]\n",
      "['W522', 0.0004184790013823658, ' Bernard Charles \"Bernie\" Ecclestone (born 28 October 1930) is a British business magnate. He was the chief executive of the Formula One Group, which manages Formula One and controls the commercial rights to the sport, and part-owns Delta Topco, the ultimate parent company of the Formula One Group. As such, he was commonly described in journalism as \\'F1 Supremo\\'. On 23 January 2017, it was announced that Ecclestone had been replaced by Chase Carey as chief executive of the Formula One Group, though he has been appointed as chairman emeritus and will act as an adviser to the board. His early involvement in the sport was as a competitor and then as a manager of drivers Stuart Lewis-Evans and Jochen Rindt. In 1972, he bought the Brabham team, which he ran for fifteen years. As a team owner he became a member of the Formula One Constructors Association. His control of the sport, which grew from his pioneering the sale of television rights in the late 1970s, was chiefly financial, but under the terms of the Concorde Agreement he and his companies also managed the administration, setup and logistics of each Formula One Grand Prix, making him one of the richest men in the UK. Ecclestone entered two Grand Prix races as a driver, during the 1958 season, but failed to qualify for either of them. Ecclestone and business partner Flavio Briatore also owned the English football club Queens Park Rangers between 2007 and 2011.\\n']\n",
      "['L379', 7.755725528113544e-05, \" BMW is a German luxury car manufacturer that has been involved in Formula One racing since the 1980s. The company has won numerous championships and races, including the Constructors' Championship in 2001 and 2002. BMW also had a successful partnership with Sauber from 1991 to 2005, during which time they won several races and podium finishes. In recent years, BMW has been involved in Formula One as an engine supplier for teams such as Sauber, Williams, and McLaren.\\n\"]\n"
     ]
    }
   ],
   "source": [
    "pip = Pipeline(3)\n",
    "\n",
    "ranked_document_list = pip.retrieve_relevant_documents(\"formula one is no fun\")\n",
    "#print(ranked_document_list)\n",
    "\n",
    "for doc in ranked_document_list:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# create validation data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract(url, destination_folder, filename):\n",
    "\n",
    "    # create folder if not exists\n",
    "    os.makedirs(destination_folder, exist_ok=True)\n",
    "\n",
    "    filename = os.path.join(destination_folder, filename)\n",
    "\n",
    "    # check if file already exists\n",
    "    if not os.path.exists(filename):\n",
    "        # download file\n",
    "        response = requests.get(url, stream=True)\n",
    "        with open(filename, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=128):\n",
    "                file.write(chunk)\n",
    "\n",
    "    # extract the zip\n",
    "    with ZipFile(filename, 'r') as zip_ref:\n",
    "        zip_ref.extractall(destination_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subset_validation_with_topic(documents_filename, qrels_filename, queries_filename, subset_docs_filename, subset_queries_filename, lines_per_file, topic):\n",
    "    documents_df = pd.read_csv(documents_filename)\n",
    "    qrels_df = pd.read_csv(qrels_filename, sep='\\t', header=None, names=['q_id', 'unused', 'doc_id', 'relevance'])\n",
    "    queries_df = pd.read_csv(queries_filename)\n",
    "\n",
    "    topic_docs_df = documents_df[documents_df['text_right'].str.contains(topic, case=False)]\n",
    "    \n",
    "    # get all q_ids where any document has the word topic in it\n",
    "    q_ids_needed = []\n",
    "    for _, row in topic_docs_df.iterrows():\n",
    "        id_right = row['id_right']\n",
    "        matching_q_ids = qrels_df[qrels_df['doc_id'] == id_right]['q_id'].tolist()\n",
    "        q_ids_needed.extend(matching_q_ids) # so no list of lists is created, multiple maches are just appended as elements\n",
    "    q_ids_needed = list(set(q_ids_needed))\n",
    "\n",
    "    # shorten the data by shortening the amount of queries\n",
    "    q_ids_needed = q_ids_needed[:lines_per_file]\n",
    "\n",
    "    # get all doc_ids out of q_rels that correspond to a q_id with topic in it\n",
    "    # this should be larger than q_ids_needed because one query has multiple docs\n",
    "    selected_doc_ids = []\n",
    "    for q_id in q_ids_needed:\n",
    "        if isinstance(q_id, list):\n",
    "            selected_doc_ids.extend(qrels_df[qrels_df['q_id'].isin(q_id)]['doc_id'].tolist())\n",
    "        else:\n",
    "            selected_doc_ids.extend(qrels_df[qrels_df['q_id'].isin([q_id])]['doc_id'].tolist())\n",
    "\n",
    "    subset_queries_df = queries_df[queries_df['id_left'].isin(q_ids_needed)]\n",
    "    subset_queries_df.to_csv(subset_queries_filename, index=False)\n",
    "\n",
    "    subset_docs_df = documents_df[documents_df['id_right'].isin(selected_doc_ids)]\n",
    "    subset_docs_df.to_csv(subset_docs_filename, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data set: https://ir-datasets.com/wikir.html\n",
    "zip_file_url = \"https://zenodo.org/record/3565761/files/wikIR1k.zip\"\n",
    "destination_folder = \"./validationDataset/\"\n",
    "filename = \"wikIR1k.zip\"\n",
    "download_and_extract(zip_file_url, destination_folder, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_subset_validation_with_topic('./validationDataset/wikIR1k/documents.csv', './validationDataset/wikIR1k/training/qrels', './validationDataset/wikIR1k/training/queries.csv', './validationDataset/validationDatset.csv', './validationDataset/validationQueries.csv', 10, ' sport ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
