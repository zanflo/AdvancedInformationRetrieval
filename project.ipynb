{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "from gpt4all import GPT4All\n",
    "import gpt4all\n",
    "#path = \"C:\\Users\\Jakob\\Downloads\\gpt4all-falcon-q4_0.gguf\"\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing of our data (creating smaller subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_file_by_lines(filename, lines_per_file,create_file):\n",
    "    with open(filename, 'r', encoding='utf-8') as file: \n",
    "        count = 0\n",
    "        current_file = open(create_file, 'w', encoding='utf-8') \n",
    "        \n",
    "        numLine = 0\n",
    "        count = 0\n",
    "        for line in file:\n",
    "            if(numLine > lines_per_file):\n",
    "                break\n",
    "            if (count % 1000) == 0:\n",
    "                current_file.write(line)\n",
    "                numLine += 1\n",
    "            count += 1    \n",
    "\n",
    "\n",
    "        current_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_file_by_lines_topic(filename, lines_per_file,create_file,topic):\n",
    "    with open(filename, 'r', encoding='utf-8') as file: \n",
    "        count = 0\n",
    "        current_file = open(create_file, 'w', encoding='utf-8') \n",
    "        \n",
    "        numLine = 0\n",
    "        count = 0\n",
    "        for line in file:\n",
    "            if(numLine > lines_per_file):\n",
    "                break\n",
    "            split_line = line.split(' ||| ')\n",
    "            if topic in split_line[1]:\n",
    "                current_file.write(line)\n",
    "                numLine += 1\n",
    "            count += 1    \n",
    "\n",
    "\n",
    "        current_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the complete titles for the queries in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_to_df_title(filename):\n",
    "    fp = open(filename, 'r', encoding='utf-8')\n",
    "    queries = []\n",
    "    for entry in fp:\n",
    "        query, _ = entry.split(' ||| ', 1)\n",
    "        queries.append(query)\n",
    "    df_queries = pd.DataFrame(queries, columns=['Query'])\n",
    "    fp.close()\n",
    "    return df_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_to_df(filename):\n",
    "    df = pd.read_csv(filename, delimiter=' \\|\\|\\| ', engine='python', header=None, names=['Title', 'Text'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = GPT4All(model_name=\"gpt4all-falcon-q4_0.gguf\",model_path=\"./model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a .txt file with the response of the llm as a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateDocWithLLM(queries):\n",
    "    fp = open(\"llmDocs.txt\", 'a')\n",
    "    index = 0\n",
    "    for title in queries['Title']:\n",
    "        query = f'give me a short summary on {title}'\n",
    "        input = query\n",
    "        #print(input)\n",
    "        with llm.chat_session():\n",
    "            response = llm.generate(input, temp=0)\n",
    "        #print(response)\n",
    "        fp.write(str(title) + \" ||| \" + str(response) + \"\\n\")\n",
    "        # if(index == 3):\n",
    "        #     break\n",
    "        index += 1\n",
    "        if(index % 50 == 0):\n",
    "            print(index)\n",
    "    fp.close()\n",
    "    print(\"finished\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split_file_by_lines('data/stemmed.txt', 1000,'data/wikipediaSubset.txt')\n",
    "#split_file_by_lines('data/raw.txt', 1000,'data/textForQueries.txt')\n",
    "#df = txt_to_df('data/wikipediaSubset.txt')\n",
    "#dfQ = txt_to_df_title('data/textForQueries.txt')\n",
    "\n",
    "\n",
    "#split_file_by_lines_topic('data/raw.txt', 1000,'data/topic.txt',' sport ')\n",
    "topic_dataframe = txt_to_df('data/topic.txt')\n",
    "\n",
    "#display(df)\n",
    "#display(dfQ)\n",
    "generateDocWithLLM(topic_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocesses the data generated by the LLM in the same way as the original data (https://github.com/tscheepers/Wikipedia-Summary-Dataset/blob/master/src/process.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_iterator():\n",
    "    with open('LLM-Output.txt', 'r') as file:\n",
    "        for line in file:\n",
    "            split = line.split(\"|||\")\n",
    "            title = split[0]\n",
    "            #join in the rare case this sequenze occurs more than once\n",
    "            text = ''.join(map(str, split[1:]))\n",
    "            yield (title, text)\n",
    "\n",
    "def preprocess_LLM_output():\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    punctiation_words = ['.', ',', ';', ':', '(', ')', '`', '\\'', '\\'\\'', '-', '–', '—', '…', '[', ']', '...', '{', '}']\n",
    "    forbidden_words = ['']\n",
    "    punctiation_chars = ['`']\n",
    "    df = {'id': [], 'title': [], 'text': [] }\n",
    "\n",
    "    counter = 0\n",
    "    for (title, text) in file_iterator():\n",
    "\n",
    "        #Tokenize\n",
    "        title = word_tokenize(title)\n",
    "        text = word_tokenize(text)\n",
    "\n",
    "        #convert words to lowercase\n",
    "        title = [t.lower() for t in title]\n",
    "        text = [t.lower() for t in text]\n",
    "\n",
    "        #remove punctuation\n",
    "        title = [t for t in title if t not in punctiation_words]\n",
    "        text = [t for t in text if t not in punctiation_words]\n",
    "\n",
    "        #remove punctuation chars\n",
    "        title = [''.join(c for c in t if c not in punctiation_chars) for t in title]\n",
    "        text = [''.join(c for c in t if c not in punctiation_chars) for t in text]\n",
    "\n",
    "        #remove forbidden words\n",
    "        title = [t for t in title if t not in forbidden_words]\n",
    "        text = [t for t in text if t not in forbidden_words]\n",
    "\n",
    "        #remove stopwords\n",
    "        title = [t for t in title if t not in stopwords_english]\n",
    "        text = [t for t in text if t not in stopwords_english]\n",
    "\n",
    "        #stemm\n",
    "        title = [stemmer.stem(t) for t in title]\n",
    "        text = [stemmer.stem(t) for t in text]\n",
    "\n",
    "        counter += 1\n",
    "        \n",
    "        df['id'].append(counter)\n",
    "        df['title'].append(title)\n",
    "        df['text'].append(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retriveing the first X documents using BM25. This is used for efficiently eliminating unreleveant documents. The resulting documaents are later reranked using more computaion intensive but also more acurate methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25:\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "        self.k1 = 1.5\n",
    "        self.b = 0.75\n",
    "        self.inverted_index = self.calc_inverted_index()\n",
    "        self.bm25 = self.calc_bm25_matrix()\n",
    "\n",
    "\n",
    "    def calc_inverted_index(self) -> dict[str, list[str]]:\n",
    "\n",
    "        inverted_index = {}\n",
    "        \n",
    "        for _, row in self.dataframe.iterrows():\n",
    "            for word in row['text']+row['title']:\n",
    "                if word not in inverted_index.keys():\n",
    "                    inverted_index[word] = []\n",
    "                    inverted_index[word].append(row['id'])\n",
    "                    \n",
    "                else:\n",
    "                    if row['doc_id'] not in inverted_index[word]:\n",
    "                        inverted_index[word].append(row['id'])\n",
    "\n",
    "        return inverted_index\n",
    "\n",
    "\n",
    "    def calc_bm25_matrix(self):\n",
    "            \n",
    "        bm25 = {}     \n",
    "\n",
    "        #construct vector for every word\n",
    "        average_document_length = self.average_document_length()\n",
    "        for _, row in self.dataframe.iterrows():\n",
    "            bm25[row['id']] = {}\n",
    "            document_unique = np.unique(row['text']+row['title'])\n",
    "            document = row['text']+row['title']\n",
    "            documnet_id = row['id']\n",
    "            for word in document_unique:\n",
    "                idf = self.calculate_idf(len(self.inverted_index[word]))\n",
    "                self.calc_bm25(word, bm25, idf, average_document_length, document, documnet_id)\n",
    "\n",
    "        return bm25\n",
    "\n",
    "    def calc_bm25(self, word, bm25, idf, average_document_length, document, documnet_id):\n",
    "        \n",
    "        term_frequency = document.count(word)\n",
    "        \n",
    "        numerator = idf * term_frequency * (self.k1 + 1)\n",
    "        denominator = term_frequency + self.k1 * (1 - self.b + ((self.b * len(document)) / average_document_length))\n",
    "        bm25[documnet_id][word] = numerator / denominator\n",
    "\n",
    "    def average_document_length(self):\n",
    "        element_counter = 0\n",
    "        word_counter = 0\n",
    "        for _, row in self.dataframe.iterrows():\n",
    "            element_counter += 1\n",
    "            word_counter += len(row['text'])+row['title']\n",
    "\n",
    "        return word_counter / element_counter\n",
    "    \n",
    "    def calculate_idf(self, amount_documents_including_word):\n",
    "        return np.log(len(self.dataframe) / amount_documents_including_word)\n",
    "    \n",
    "    def preprocess_query(self, query: str):\n",
    "        stemmer = PorterStemmer()\n",
    "        stopwords_english = stopwords.words('english')\n",
    "        punctiation_words = ['.', ',', ';', ':', '(', ')', '`', '\\'', '\\'\\'', '-', '–', '—', '…', '[', ']', '...', '{', '}']\n",
    "        forbidden_words = ['']\n",
    "        punctiation_chars = ['`']\n",
    "\n",
    "        #Tokenize\n",
    "        query = word_tokenize(query)\n",
    "\n",
    "        #convert words to lowercase\n",
    "        query = [t.lower() for t in query]\n",
    "\n",
    "        #remove punctuation\n",
    "        query = [t for t in query if t not in punctiation_words]\n",
    "\n",
    "        #remove punctuation chars\n",
    "        query = [''.join(c for c in t if c not in punctiation_chars) for t in query]\n",
    "\n",
    "        #remove forbidden words\n",
    "        query = [t for t in query if t not in forbidden_words]\n",
    "\n",
    "        #remove stopwords\n",
    "        query = [t for t in query if t not in stopwords_english]\n",
    "\n",
    "        #stemm\n",
    "        query = [stemmer.stem(t) for t in query]\n",
    "\n",
    "        return query\n",
    "\n",
    "\n",
    "    def retrieve_relevance(self, query: str, k: int) -> dict[str, float]:\n",
    "        ### ADD YOUR CODE (BEGIN) ###\n",
    "\n",
    "        query = self.preprocess_query(query)\n",
    "\n",
    "        #filter out word not contained in any document for efficiency\n",
    "        query = [word for word in query if word in self.inverted_index.keys()]\n",
    "\n",
    "        bm25_current_query = {}\n",
    "\n",
    "        for _, row in self.dataframe.iterrows():\n",
    "            bm25_current_query[row['id']] = []\n",
    "            doc_id = row['id']\n",
    "            for word in query:\n",
    "                if word in self.bm25[doc_id]:\n",
    "                    bm25_current_query[doc_id].append(self.bm25[doc_id][word])\n",
    "                else: \n",
    "                    bm25_current_query[doc_id].append(0)\n",
    "\n",
    "        for key, value in bm25_current_query.items():\n",
    "            bm25_current_query[key] = sum(value)\n",
    "\n",
    "        sorted_bm25 = sorted(bm25_current_query.items(), key=lambda x:x[1], reverse=True)\n",
    "\n",
    "        sorted_bm25 = sorted_bm25[:k]\n",
    "        bm25_current_query = dict(sorted_bm25)\n",
    "\n",
    "        return bm25_current_query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# merge llm output and documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_files(file1_path, file2_path, merged_file_path):\n",
    "    try:\n",
    "        with open(file1_path, 'r') as file1, open(file2_path, 'r') as file2:\n",
    "            content1 = file1.read()\n",
    "            content2 = file2.read()\n",
    "\n",
    "            merged_content = content1 + content2\n",
    "            \n",
    "        with open(merged_file_path, 'w') as merged_file:\n",
    "            merged_file.write(merged_content)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Error: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_files('llmDocs.txt', './data/topic.txt', 'mixedDocs.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Rerank using MonoBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download model\n",
    "\n",
    "model_name = \"castorini/monobert-large-msmarco\"\n",
    "model_path = \"./model\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.save_pretrained(model_path)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./model\"\n",
    "\n",
    "local_model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "local_tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevance_bert(query, documents):\n",
    "    relevances = []\n",
    "    for document in documents:\n",
    "\n",
    "        inputs = tokenizer(query, document, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "        # predictions    \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # e.g.:\n",
    "        # outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-1.8822,  2.8848]]), hidden_states=None, attentions=None)\n",
    "        # logits: tensor([[-1.8822,  2.8848]])\n",
    "\n",
    "        # re-scale to [0,1] and so that sum == 1\n",
    "        relevance = torch.nn.functional.softmax(logits, dim=1)\n",
    "        relevances.append([document, relevance[0].tolist()[1]])\n",
    "\n",
    "    return relevances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_relevance(relevances):\n",
    "    relevances = sorted(relevances, reverse=True, key=lambda x: x[1])\n",
    "    return relevances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the capital of France?\"\n",
    "documents = [\"Paris is the capital of France.\",\n",
    "             \"birds like France.\",\n",
    "             \"snakes eat gras\",\n",
    "             \"Graz is the capital of France.\",\n",
    "             \"France is capital paris\",\n",
    "             \"paris is capital france\",\n",
    "             \"France capital paris egg\"]\n",
    "\n",
    "relevances = get_relevance_bert(query, documents)\n",
    "relevances = order_relevance(relevances)\n",
    "\n",
    "for relevance in relevances:\n",
    "    print(relevance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
