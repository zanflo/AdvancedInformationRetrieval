# Content Accuracy of the gpt4all-falcon model
Advanced Information Retrieval group project 2023/24   
Group 03 with members:  

**Patrick Hanfstingl**  
**Florian Zanotti**  
**Jakob Zenz**
___
___
___
___

# Project Idea
In this project we investigate whether an open source Large Language Model (LLM), in particular gpt4all-falcon, can deliver relevant results in terms of Information Retrieval.
___

# Implementation
Overview of the implementation:
1) Get a dataset containing the titles we use as queries and the corresponding documents, we chose a Wikipedia subset.
2) Give the LLM one query at a time as input and let it generate a document from scratch.
3) Merge the original documents and the documents generated by the LLM, so that the dataset contains an additional 50% of documents generated by the LLM.
4) Retrieve 50 to 100 documents using BM25 (to speed up retrieval)
5) Re-rank these documents using a transformer, specifically MonoBert
6) Sort out documents that have a very low relevance score, they are basically not relevant matches.
7) Now we get a corresponding mixed ranking of LLM and non-LLM documents, which directly translates to the performance of the LLM compared to human written wikipedia articles. Then we calculate some metrics/statistics about the quantity and rank of LLM documents compared to non-LLM documents.
___

## Choosing the LLM
Gpt4all-falcon (https://gpt4all.io/index.html) has the following features
- Relatively small size at ~4 GB
- Requires only 8 GB of RAM
- Described as the fastest of all gpt4all models
- Has qualitative output that suits our task 
- Performs well on average in various benchmarks compared to other possible models
___ 

## Choosing the Transformer
MonoBert (https://huggingface.co/castorini/monobert-large-msmarco) has the following features:
- MonoBert adapts Bert for relevance classification 
    - Bert is a pre-trained natural language processing model using a transformer architecture.
    - For more information on how MonoBert works, see the original paper at https://arxiv.org/pdf/1901.04085.pdf or a paper that briefly summarizes the functionality at https://arxiv.org/pdf/2010.06467.pdf, chapter 3.2.1.
- Open source and well known
- Easy to use 
- Performs very well on our validation data
___

## Choosing the Dataset
We needed a dataset with the following characteristics:
- A collection of general, broad knowledge so that we could test the general performance of gpt4all-falcon.
- Has queries or some text that we could use as queries
- Not too large, as we have very limited resources

A suitable dataset is the Wikipedia summary dataset (https://github.com/tscheepers/Wikipedia-Summary-Dataset). This contains many summaries of Wikipedia pages that are short in length and therefore do not require too many resources. It contains general, broad knowledge. It also has an extra column that contains the titles that we can use as queries.

This dataset still contains too many rows for our capabilities, so we created a subset of it by using only documents that contain the word 'sport' in them. So we basically have a subset with a sports theme.
___
## Validation of BM25 and MonoBert
It is important to validate the implementations of BM25 and monoBert that we use without gpt4all-falcon, so that we can exclude these two as interfering factors in our test. If BM25 and monoBert perform well in the validation, the results are only influenced by the performance of gpt4all-falcon. With this validation we can draw some conclusions about the performance of gpt4all-falcon.

### Choice of validation dataset
The dataset we use must have the following characteristics:
- Similar content to the dataset we use for testing
- Ground truths such as query relevancies (qrels)
- Cannot be too large because of limited resources

The dataset we have chosen that fulfills these criteria is WikIR1k (https://ir-datasets.com/wikir.html). This dataset contains Wikipedia articles with different purpose knowledge, which is identical to the dataset we use for testing. This dataset also contains queries and the corresponding qrels

Note that this set cannot be used to generate the LLM answers, as the queries given are only excerpts from the documents themselves and not topics that an LLM can give answer to.

### Generation of the Subset
The set is too large for our resources, so we took a subset of it. The creation of the subset is worth mentioning. We took all documents containing the word sport. We then added the corresponding queries that we found through qrels to the subset of queries. Then we took all the documents that were a ground truth of the queries in the queries subset and added them to the documents' subset. This ensures that it is possible for BM25 and monoBert to get a perfect match, but we do not falsify the outcome.

### Outcome
TODO
___
## Findings
TODO
![Alternativer Text](out_2.png)
___
## Future Improvements
Although our results are significant, there are a few things that could be improved:
- Select a different dataset for testing. MonoBert ranks the documents according to the semantics of the queries. The results of the findings could probably be more specific if the queries were semantically more meaningful/detailed.
- Get more resources to:
    - use larger datasets to exclude outliers
    - test the model not only on sports documents but also on general knowledge.
