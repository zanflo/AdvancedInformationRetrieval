2023/24 Advanced Information Retrieval group project
___
# Project Idea
In this project we investigate whether an open source Large Language Model (LLM), in particular gpt4all-falcon, can deliver relevant results in terms of Information Retrieval.
___
# Implementation
Overview of the implementation:
1) Get a dataset containing the titles we use as queries and the corresponding documents, we chose a Wikipedia subset.
2) Give the LLM one query at a time as input and let it generate a document from scratch.
3) Merge the original documents and the documents generated by the LLM, so that the dataset contains an additional 50% of documents generated by the LLM.
4) Retrieve 50 to 100 documents using BM25 (to speed up retrieval)
5) Re-rank these documents using a transformer, specifically MonoBert
6) Sort out documents that have a very low relevance score, they are basically not relevant matches.
7) Now we get a corresponding mixed ranking of LLM and non-LLM documents, which directly translates to the performance of the LLM compared to human written wikipedia articles. Then we calculate some metrics/statistics about the quantity and rank of LLM documents compared to non-LLM documents.
__
## The Choice of the LLM
Gpt4all-falcon (https://gpt4all.io/index.html) has the following features
- Relatively small size at ~4 GB
- Requires only 8 GB of RAM
- Described as the fastest of all gpt4all models
- Has qualitative output that suits our task 
- Performs well on average in various benchmarks compared to other possible models
___ 
## The Choice of the Transformer
## The Choice of Data Set
## Validation of BM25 and MonoBert
### The Choice of the Validation Data Set
### Generation of the Subset
### Outcomes
## Findings