{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation of BM25 and BERT\n",
    "We created this seperate notebook for the validation so that the main project does not get too cluttered, also some changes to read in the files had to be done, which would clutter the main file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jakob\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Jakob\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "c:\\Users\\Jakob\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "#from gpt4all import GPT4All\n",
    "#import gpt4all\n",
    "#path = \"C:\\Users\\Jakob\\Downloads\\gpt4all-falcon-q4_0.gguf\"\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Validation Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract(url, destination_folder, filename):\n",
    "\n",
    "    # create folder if not exists\n",
    "    os.makedirs(destination_folder, exist_ok=True)\n",
    "\n",
    "    filename = os.path.join(destination_folder, filename)\n",
    "\n",
    "    # check if file already exists\n",
    "    if not os.path.exists(filename):\n",
    "        # download file\n",
    "        response = requests.get(url, stream=True)\n",
    "        with open(filename, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=128):\n",
    "                file.write(chunk)\n",
    "\n",
    "    # extract the zip\n",
    "    with ZipFile(filename, 'r') as zip_ref:\n",
    "        zip_ref.extractall(destination_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data set: https://ir-datasets.com/wikir.html\n",
    "zip_file_url = \"https://zenodo.org/record/3565761/files/wikIR1k.zip\"\n",
    "destination_folder = \"./validationDataset/\"\n",
    "filename = \"wikIR1k.zip\"\n",
    "download_and_extract(zip_file_url, destination_folder, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subset_validation_with_topic(documents_filename, qrels_filename, queries_filename, subset_docs_filename, subset_queries_filename, lines_per_file, topic):\n",
    "    documents_df = pd.read_csv(documents_filename)\n",
    "    qrels_df = pd.read_csv(qrels_filename, sep='\\t', header=None, names=['q_id', 'unused', 'doc_id', 'relevance'])\n",
    "    queries_df = pd.read_csv(queries_filename)\n",
    "\n",
    "    topic_docs_df = documents_df[documents_df['text_right'].str.contains(topic, case=False)]\n",
    "    \n",
    "    # get all q_ids where any document has the word topic in it\n",
    "    q_ids_needed = []\n",
    "    for _, row in topic_docs_df.iterrows():\n",
    "        id_right = row['id_right']\n",
    "        matching_q_ids = qrels_df[qrels_df['doc_id'] == id_right]['q_id'].tolist()\n",
    "        q_ids_needed.extend(matching_q_ids) # so no list of lists is created, multiple maches are just appended as elements\n",
    "    q_ids_needed = list(set(q_ids_needed))\n",
    "\n",
    "    # shorten the data by shortening the amount of queries\n",
    "    q_ids_needed = q_ids_needed[:lines_per_file]\n",
    "\n",
    "    # get all doc_ids out of q_rels that correspond to a q_id with topic in it\n",
    "    # this should be larger than q_ids_needed because one query has multiple docs\n",
    "    selected_doc_ids = []\n",
    "    for q_id in q_ids_needed:\n",
    "        if isinstance(q_id, list):\n",
    "            selected_doc_ids.extend(qrels_df[qrels_df['q_id'].isin(q_id)]['doc_id'].tolist())\n",
    "        else:\n",
    "            selected_doc_ids.extend(qrels_df[qrels_df['q_id'].isin([q_id])]['doc_id'].tolist())\n",
    "\n",
    "    subset_queries_df = queries_df[queries_df['id_left'].isin(q_ids_needed)]\n",
    "    subset_queries_df.to_csv(subset_queries_filename, index=False)\n",
    "\n",
    "    subset_docs_df = documents_df[documents_df['id_right'].isin(selected_doc_ids)]\n",
    "    subset_docs_df.to_csv(subset_docs_filename, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_subset_validation_with_topic('./validationDataset/wikIR1k/documents.csv', './validationDataset/wikIR1k/training/qrels', './validationDataset/wikIR1k/training/queries.csv', './validationDataset/validationDatset.csv', './validationDataset/validationQueries.csv', 50, ' sport ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BM25\n",
    "Same as in main project but with some changes to data input because of a different format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_iterator(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            split = line.split(\",\")\n",
    "            id = split[0]\n",
    "            #join in the rare case this sequenze occurs more than once\n",
    "            text = ''.join(map(str, split[1:]))\n",
    "            yield (id, text)\n",
    "\n",
    "def preprocess_Data(filename):\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    \n",
    "    df = {}\n",
    "\n",
    "    counter = 1\n",
    "    for (id, text) in file_iterator(filename):\n",
    "        \n",
    "        #Tokenize\n",
    "        text = word_tokenize(text)\n",
    "\n",
    "        #convert words to lowercase\n",
    "        text = [t.lower() for t in text]\n",
    "\n",
    "        #remove punctuation\n",
    "        text = [t for t in text if t.isalnum() or t.isspace()]\n",
    "\n",
    "        #remove stopwords\n",
    "        text = [t for t in text if t not in stopwords_english]\n",
    "\n",
    "        #stemm\n",
    "        text = [stemmer.stem(t) for t in text]\n",
    "        \n",
    "        df[id] = text\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.k1 = 1.5\n",
    "        self.b = 0.75\n",
    "        self.inverted_index = self.calc_inverted_index()\n",
    "        self.bm25 = self.calc_bm25_matrix()\n",
    "\n",
    "\n",
    "    def calc_inverted_index(self) -> dict[str, list[str]]:\n",
    "\n",
    "        inverted_index = {}\n",
    "        \n",
    "        for doc_id, words in self.data.items():\n",
    "            for word in words:\n",
    "                if word not in inverted_index.keys():\n",
    "                    inverted_index[word] = []\n",
    "                    inverted_index[word].append(doc_id)\n",
    "                    \n",
    "                else:\n",
    "                    if doc_id not in inverted_index[word]:\n",
    "                        inverted_index[word].append(doc_id)\n",
    "\n",
    "        return inverted_index\n",
    "\n",
    "\n",
    "    def calc_bm25_matrix(self):\n",
    "            \n",
    "        bm25 = {}     \n",
    "\n",
    "        #construct vector for every word\n",
    "        average_document_length = self.average_document_length()\n",
    "        for doc_id, words in self.data.items():\n",
    "            bm25[doc_id] = {}\n",
    "            document_unique = np.unique(words)\n",
    "            document = words\n",
    "            documnet_id = doc_id\n",
    "            for word in document_unique:\n",
    "                idf = self.calculate_idf(len(self.inverted_index[word]))\n",
    "                self.calc_bm25(word, bm25, idf, average_document_length, document, documnet_id)\n",
    "\n",
    "        return bm25\n",
    "\n",
    "    def calc_bm25(self, word, bm25, idf, average_document_length, document, documnet_id):\n",
    "        \n",
    "        term_frequency = document.count(word)\n",
    "        \n",
    "        numerator = idf * term_frequency * (self.k1 + 1)\n",
    "        denominator = term_frequency + self.k1 * (1 - self.b + ((self.b * len(document)) / average_document_length))\n",
    "        bm25[documnet_id][word] = numerator / denominator\n",
    "\n",
    "    def average_document_length(self):\n",
    "        element_counter = 0\n",
    "        word_counter = 0\n",
    "        for _, words in self.data.items():\n",
    "            element_counter += 1\n",
    "            word_counter += len(words)\n",
    "\n",
    "        return word_counter / element_counter\n",
    "    \n",
    "    def calculate_idf(self, amount_documents_including_word):\n",
    "        return np.log(len(self.data) / amount_documents_including_word)\n",
    "    \n",
    "    def preprocess_query(self, query: str):\n",
    "        stemmer = PorterStemmer()\n",
    "        stopwords_english = stopwords.words('english')\n",
    "\n",
    "        #Tokenize\n",
    "        query = word_tokenize(query)\n",
    "\n",
    "        #convert words to lowercase\n",
    "        query = [t.lower() for t in query]\n",
    "\n",
    "        #remove punctuation\n",
    "        query = [t for t in query if t.isalnum or t.isspace()]\n",
    "\n",
    "        #remove stopwords\n",
    "        query = [t for t in query if t not in stopwords_english]\n",
    "\n",
    "        #stemm\n",
    "        query = [stemmer.stem(t) for t in query]\n",
    "\n",
    "        return query\n",
    "\n",
    "\n",
    "    def retrieve_relevance(self, query: str, k: int) -> dict[str, float]:\n",
    "        ### ADD YOUR CODE (BEGIN) ###\n",
    "\n",
    "        query = self.preprocess_query(query)\n",
    "\n",
    "        #filter out word not contained in any document for efficiency\n",
    "        query = [word for word in query if word in self.inverted_index.keys()]\n",
    "\n",
    "        bm25_current_query = {}\n",
    "\n",
    "        for doc_id, words in self.data.items():\n",
    "            bm25_current_query[doc_id] = []\n",
    "            for word in query:\n",
    "                if word in self.bm25[doc_id]:\n",
    "                    bm25_current_query[doc_id].append(self.bm25[doc_id][word])\n",
    "                else: \n",
    "                    bm25_current_query[doc_id].append(0)\n",
    "\n",
    "        for key, value in bm25_current_query.items():\n",
    "            bm25_current_query[key] = sum(value)\n",
    "\n",
    "        sorted_bm25 = sorted(bm25_current_query.items(), key=lambda x:x[1], reverse=True)\n",
    "\n",
    "        sorted_bm25 = sorted_bm25[:k]\n",
    "        bm25_current_query = dict(sorted_bm25)\n",
    "\n",
    "        return bm25_current_query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MonoBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./model\\\\tokenizer_config.json',\n",
       " './model\\\\special_tokens_map.json',\n",
       " './model\\\\vocab.txt',\n",
       " './model\\\\added_tokens.json',\n",
       " './model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download model\n",
    "\n",
    "model_name = \"castorini/monobert-large-msmarco\"\n",
    "model_path = \"./model\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.save_pretrained(model_path)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./model\"\n",
    "\n",
    "local_model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "local_tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevance_bert(query, documents):\n",
    "    relevances = []\n",
    "    for document in documents:\n",
    "        \n",
    "        inputs = tokenizer(query, document[1], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "        # predictions    \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # e.g.:\n",
    "        # outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-1.8822,  2.8848]]), hidden_states=None, attentions=None)\n",
    "        # logits: tensor([[-1.8822,  2.8848]])\n",
    "\n",
    "        # re-scale to [0,1] and so that sum == 1\n",
    "        relevance = torch.nn.functional.softmax(logits, dim=1)\n",
    "        relevances.append([document[0], relevance[0].tolist()[1], document[1]])\n",
    "\n",
    "    return relevances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_relevance(relevances):\n",
    "    relevances = sorted(relevances, reverse=True, key=lambda x: x[1])\n",
    "    return relevances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    def __init__(self, amount_bm25_fetched_documents, amount_of_queries):\n",
    "        data=self.getData()\n",
    "        self.bm25 = BM25(data)\n",
    "        self.documents = self.getCompleteFiles()\n",
    "        self.amount_bm25_fetched_documents = amount_bm25_fetched_documents\n",
    "        self.queries = self.get_queries_from_doc()\n",
    "        self.amount_of_queries = amount_of_queries\n",
    "\n",
    "    def getData(self):\n",
    "        data = preprocess_Data('./validationDataset/validationDatset.csv')\n",
    "        return data\n",
    "    \n",
    "    def getCompleteFiles(self):\n",
    "        documents = {}\n",
    "\n",
    "        #read LLM generated Documents\n",
    "        with open('./validationDataset/validationDatset.csv', 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                split = line.split(\",\")\n",
    "                id = split[0]\n",
    "                #join in the rare case this sequenze occurs more than once\n",
    "                text = ''.join(map(str, split[1:]))\n",
    "                documents[id] = text\n",
    "        \n",
    "        return documents\n",
    "    \n",
    "    def retrieve_relevant_documents_with_BERT(self, query, documents):\n",
    "        relevances = get_relevance_bert(query, documents)\n",
    "        relevances = order_relevance(relevances)   \n",
    "        return relevances \n",
    "    \n",
    "    def retrieve_relevant_documents(self, query):\n",
    "\n",
    "        #retrieve amount_bm25_fetched_documents documents using fast bm25\n",
    "        bm25_docs = self.bm25.retrieve_relevance(query, self.amount_bm25_fetched_documents)\n",
    "\n",
    "        #fetch the non preprocessed documents retrieved by bm25\n",
    "        retrieved_documents = []\n",
    "        for doc_id in bm25_docs:\n",
    "            retrieved_documents.append([doc_id, self.documents[doc_id]])\n",
    "\n",
    "        #rerank these documents using BERT\n",
    "        ranked_document_list = self.retrieve_relevant_documents_with_BERT(query, retrieved_documents)\n",
    "\n",
    "        return ranked_document_list\n",
    "\n",
    "    def get_queries_from_doc(self):\n",
    "        queries_list = []\n",
    "        with open('./validationDataset/validationQueries.csv', 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                split = line.split(\",\")\n",
    "                text = split[1]\n",
    "                text = text.strip()\n",
    "                queries_list.append(text)\n",
    "            queries_list = queries_list[1:]    \n",
    "        return queries_list\n",
    "    \n",
    "    def run_queries(self):\n",
    "        ret = []\n",
    "        for query in self.queries[:self.amount_of_queries]:\n",
    "            ret.append((query, self.retrieve_relevant_documents(query)))\n",
    "\n",
    "        return ret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation\n",
    "Validate BM25, Bert, and the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Validation:\n",
    "    def __init__(self, valDocs,valQueries,qrels):\n",
    "        self.queries = self.createQueriesDF(valQueries)\n",
    "        self.topicQrels = self.createQrels(valQueries,qrels)\n",
    "        self.bm = BM25(preprocess_Data('./validationDataset/validationDatset.csv'))\n",
    "        self.pip = Pipeline(5,len(self.queries))\n",
    "\n",
    "    def createQueriesDF(self,valQueries):\n",
    "        queries = {}\n",
    "        fp = open(valQueries, 'r')\n",
    "        for line in fp:\n",
    "            parts = line.strip().split(',')\n",
    "            query_id, title = parts\n",
    "            queries[query_id] = title\n",
    "        fp.close()     \n",
    "        del queries[list(queries)[0]]           \n",
    "        return queries\n",
    "\n",
    "    def createQrels(self,valQueries,qrels):\n",
    "        qrelsDict = self.queries.copy()\n",
    "        for key in qrelsDict:\n",
    "            qrelsDict[key] = []\n",
    "        fp = open(qrels, 'r')\n",
    "        for line in fp:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 4:\n",
    "                query_id, _, doc_id, _ = parts\n",
    "                if query_id in qrelsDict:\n",
    "                    qrelsDict[query_id].append(doc_id)\n",
    "        return qrelsDict\n",
    "    \n",
    "    def generatePredsBm25(self,k: int = 5):\n",
    "        predictions = {}\n",
    "        for id, query in self.queries.items():\n",
    "            preds = self.bm.retrieve_relevance(query,k)\n",
    "            predictions[id] = preds\n",
    "        return predictions\n",
    "    \n",
    "    def generatePredsPipe(self,k: int = 5):\n",
    "        preds = {}\n",
    "        predictions = self.pip.run_queries()\n",
    "        for i in predictions:\n",
    "            id = 0\n",
    "            # print(i[1])\n",
    "            for key,val in self.queries.items():\n",
    "                if val == i[0]:\n",
    "                    id = key\n",
    "                    preds[id] = {}\n",
    "                    break\n",
    "            for j in i[1]:\n",
    "                preds[id][j[0]] = j[1]      \n",
    "\n",
    "        return preds\n",
    "\n",
    "    def f1atK(self, predictions: dict[str, list[str]], k: int = 5):\n",
    "        counter = 0\n",
    "        recall = 0\n",
    "        precision = 0\n",
    "        \n",
    "        for key, value in self.topicQrels.items():\n",
    "            if key not in predictions:\n",
    "                continue\n",
    "            counter += 1\n",
    "            true_positives = 0\n",
    "            false_negatives = 0\n",
    "            false_positives = 0\n",
    "            \n",
    "            for prediction in predictions[key]:\n",
    "                if prediction in value:\n",
    "                    true_positives += 1\n",
    "                else:\n",
    "                    false_positives += 1\n",
    "\n",
    "            false_negatives = (k - true_positives) if len(value) > k else (len(value) - true_positives)\n",
    "\n",
    "            precision += true_positives / (true_positives + false_positives)\n",
    "            recall += true_positives / (true_positives + false_negatives)\n",
    "\n",
    "        recall /= counter\n",
    "        precision /= counter\n",
    "\n",
    "        #F1@k value\n",
    "        return (2 * precision * recall) / (precision + recall)\n",
    "    \n",
    "        \n",
    "    \n",
    "    def ncdgk(self, predictions: dict[str, list[str]], k: int = 5):\n",
    "        counter = 0\n",
    "        ndcg = 0\n",
    "        \n",
    "        for key, value in self.topicQrels.items():\n",
    "            if key not in predictions:\n",
    "                continue\n",
    "\n",
    "            counter += 1\n",
    "            \n",
    "            rel_i = []\n",
    "            for prediction in predictions[key]:\n",
    "                if prediction in value:\n",
    "                    rel_i.append(1)\n",
    "                else:\n",
    "                    rel_i.append(0)\n",
    "\n",
    "            dcg = 0\n",
    "            for index, rel in enumerate(rel_i):\n",
    "                dcg += rel / np.log(index+2)\n",
    "\n",
    "            rel_i.sort(reverse=True)\n",
    "\n",
    "            idcg = 0\n",
    "            for index, rel in enumerate(rel_i):\n",
    "                idcg += rel / np.log(index+2)\n",
    "\n",
    "            if idcg != 0:\n",
    "                ndcg += dcg / idcg\n",
    "\n",
    "        return ndcg / counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25\n",
      "F1@k score: 0.8240000000000001\n",
      "nDCG@k score: 0.9815848099359128\n",
      "------------------------\n",
      "BM25 + MonoBert\n",
      "F1@k score: 0.8240000000000001\n",
      "nDCG@k score: 0.988581683665839\n"
     ]
    }
   ],
   "source": [
    "val = Validation('./validationDataset/validationDatset.csv','./validationDataset/validationQueries.csv','./validationDataset/wikIR1k/training/qrels')\n",
    "#print(val.queries.iloc[0])\n",
    "#print(val.queries['qID'][0])\n",
    "#print(val.queries)\n",
    "#print(val.topicQrels)\n",
    "preds = val.generatePredsBm25()\n",
    "f1atk = val.f1atK(preds)\n",
    "ncdgk = val.ncdgk(preds)\n",
    "preds2 = val.generatePredsPipe()\n",
    "f1atk2 = val.f1atK(preds2)\n",
    "ncdgk2 = val.ncdgk(preds2)\n",
    "#print(preds)\n",
    "print(\"BM25\")\n",
    "print(\"F1@k score: \" + str(f1atk))\n",
    "print(\"nDCG@k score: \"+ str(ncdgk))\n",
    "print(\"------------------------\")\n",
    "#print(preds2)\n",
    "print(\"BM25 + MonoBert\")\n",
    "print(\"F1@k score: \" + str(f1atk2))\n",
    "print(\"nDCG@k score: \"+ str(ncdgk2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
